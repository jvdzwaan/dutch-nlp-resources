Models
======

List of pretrained word embeddings and other models.

Word embeddings
---------------

* `fastText <https://fasttext.cc/docs/en/crawl-vectors.html>`_: fastText word vectors trained on Common Crawl and Wikipedia.
* `Word2Vec <https://github.com/clips/dutchembeddings>`_: Word2Vec vectors trained by CLIPS on diffent Dutch corpora.
* `Word2Vec <http://vectors.nlpl.eu/repository/>`_: Word2Vec vectors trained by the Nordic Language Processing Laboratory (NLPL) on the CoNLL17 corpus.
* `ConceptNet Numberbatch <https://github.com/commonsense/conceptnet-numberbatch>`_: multilingual word embeddings in the same semantic space. Built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.

ULMFiT
------

* `Leiden fastai ULMFiT model <http://textdata.nl>`_: Trained on Wikipedia by the Text Mining and Retrieval research group at Leiden University.

BERT
----

* `BERTje <https://github.com/wietsedv/bertje>`_: Dutch pre-trained BERT model developed at the University of Groningen.
* `RobBERT <https://pieter.ai/robbert/>`_: Dutch BERT model using RoBERTa's pre-training, developed by KU Leuven. Trained on scraped data from the Dutch section of the OSCAR corpus. `4 smaller, distilled models <https://huggingface.co/DTAI-KULeuven/robbertje-1-gb-non-shuffled>`_ are also available.
* `BERT-NL <http://textdata.nl>`_: cased and uncased BERT model trained on the SoNaR corpus by the Text Mining and Retrieval research group at Leiden University.
* `mBERT <https://github.com/google-research/bert/blob/master/multilingual.md>`_: Multilingual BERT model.
